{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单来说，tensor可以看做是一个多维数组，进行数据的存储和运算。常用的张量类型有FloatTensor(实数型)和LongTensor(长整数型)，可以分别存储数据金额索引。初始化张量既可以将数组转化为tensor，也可以直接利用PyTorch自带的函数进行随机初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个大小为2*3的实数型张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1.2, 3.4, 5], [3, 6, 7.4]]);a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个大小为5*6的实数型张量，每个元素根据正态分布N(0, 1)随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.9777,  0.0945,  2.2145, -0.9826,  0.7154, -0.1066],\n        [ 0.4646,  0.9623, -0.6247,  0.5775, -0.0253,  0.4498],\n        [-1.2845, -0.4911,  0.0126,  0.2117, -0.4164,  0.3978],\n        [-0.2474,  0.4484, -0.7149, -0.8894,  2.7722,  0.2696],\n        [-0.5786,  0.2264,  0.6081,  0.0043,  0.7896,  1.2771]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "b = torch.randn(5, 6);b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将a第0行第2列的元素改为5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 2] = 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.2000, 3.4000, 5.6000],\n        [3.0000, 6.0000, 7.4000]])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算与求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch框架最大的好处在于可以根据张量的运算动态生成计算图(computation graph)，从而进行自动微分和求导。在PyTorch中，如果一个张量中的参数要计算导数，需要设置该张量的requires_grad属性为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "np.array: [1.3  0.5  1.9  2.45]\ntensor: tensor([1.3000, 0.5000, 1.9000, 2.4500], dtype=torch.float64, requires_grad=True)\n"
    }
   ],
   "source": [
    "x = np.array([1.3, 0.5, 1.9, 2.45])\n",
    "print('np.array:', x)\n",
    "t = torch.tensor(x, requires_grad=True)\n",
    "print('tensor:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1.])"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "a = torch.ones(1);a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-d42f3a4a9ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 将a放入GPU，本机无N卡\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "a = a.cuda()  # 将a放入GPU，本机无N卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置a需要计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3 * a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x.requires_grad  # 因为a需要计算导数，所以x需要计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有参数的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算a的导数，为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([3.])"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch的软件包nn中包含了绝大部分常用的网络结构。需要注意的是，在实际网络计算中需要将数据分批进行处理，一个批次的数据可以统一计算并求导，从而大大加快运算速度。因此，PyTorch框架中许多网络的输入默认有一维是批次大小，即batch_size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch中使用nn.Linear命令实现两层神经元之间的全连接。nn.Linear(in_feature, out_feature, bias=True)表示前一层有in_feature个神经元，下一层有out_feature个神经元，bias表示是否需要截距（默认为True）。一个nn.Linear中有in_feature*out_feature+out_feature个权重（包括截距）。全连接层的输入张量需要最后一维大小是in_feature，输出张量最后一维大小是out_feature。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四层神经网络，输入层大小为30，两个隐藏层大小分别为50和70，输出层大小为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(in_features=30, out_features=50)\n",
    "linear2 = nn.Linear(in_features=50, out_features=70)\n",
    "linear3 = nn.Linear(in_features=70, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10组输入数据作为一批次（batch），每个输入为30维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 30])"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "x = torch.randn(10, 30);x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10组输出数据，每一个输出为1维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = linear3(linear2(linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.0313],\n        [ 0.3675],\n        [ 0.2525],\n        [ 0.3356],\n        [-0.0732],\n        [ 0.7092],\n        [ 0.4141],\n        [ 0.1981],\n        [ 0.2590],\n        [ 0.1702]], grad_fn=<ThAddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 丢弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch中使用nn.Dropout命令实现Dropout层。nn.Dropout(p=0.3)表示Dropout置零概率为p=0.3（默认为0.5）。其输入可以是任意维的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input: tensor([[-0.8905, -0.7364],\n        [-0.0502, -0.7841],\n        [-2.2181,  1.4047],\n        [-0.1035,  0.7474],\n        [-0.4884, -1.3764]])\ninput_dropout: tensor([[-1.2721, -1.0520],\n        [-0.0000, -1.1201],\n        [-3.1687,  2.0067],\n        [-0.1479,  1.0678],\n        [-0.6977, -0.0000]])\n"
    }
   ],
   "source": [
    "layer = nn.Dropout(p=0.3)  # Dropout层，置零概率为0.1\n",
    "input = torch.randn(5, 2)\n",
    "print('input: {}'.format(input))\n",
    "output = layer(input)\n",
    "print('input_dropout: {}'.format(output))  # 可以看到有的数被置为0了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch中使用nn.Conv2d命令实现卷积神经网络。nn.Conv2d(in_channels, out_channels, kernel_size, bias=True)表示有in_channels个输入通道、out_channels个输出通道，过滤器大小为kernel_size*kernel_size，bise表示卷积中是否需要加截距。输入维度batch*in_channels*height*width的张量，输出为维度是batch*out_channels*height_out_width_out的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积神经网络，输入通道有1个，输出通道有3个，过滤器大小为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10个输入数据作为一批次(batch)，每一个输入为单通道32*32矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 1, 32, 32])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "x = torch.randn(10, 1, 32, 32);x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y维度为10*3*28*28，表示输出10组数据，每一个输出为3通道28*28矩阵（28=32-5+1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([10, 3, 28, 28])"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "y = conv(x); y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch中使用nn.GRU命令可以实现基于GRU的循环神经网络，使用nn.LSTM命令可以实现基于LSTM的循环神经网络。\n",
    "# 以GRU为例，nn.GRU(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False)，\n",
    "# 表示输入的每个元素xt的维度是input_size，状态ht的维度是hidden_size，一共有num_layers层（默认为1层），bias表示是否需要加截距（默认为True），batch_first表示batch的维度是否为第0维（默认为第1维）、Dropout的概率（默认是0）、是否是双向RNN（默认是False）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双层GRU输入元素维度是10，状态维度是20，batch是第1维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(input_size=10, hidden_size=20, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一批次共3个序列，每个序列长度为5，维度为10，注意batch是第1维的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 1.6564,  0.8530, -1.7247,  1.3092,  1.4854, -0.1342, -0.2576,\n           0.2223, -0.3561,  1.0511],\n         [ 0.0946, -0.0584, -0.8242, -0.5478, -1.1778, -0.2225, -1.2146,\n          -1.2610,  0.7019,  0.9740],\n         [-0.5392,  0.4369,  1.0690,  1.8458,  1.1952,  0.3555,  0.5510,\n          -0.0762, -2.2188, -0.7653]],\n\n        [[ 1.5738, -1.2145,  0.0223, -0.3202,  1.3587,  0.1080, -1.7904,\n          -1.0863, -1.0665,  1.2561],\n         [ 0.4089, -0.9534, -0.1992,  1.5218, -0.9697,  1.1563, -0.2080,\n           1.4709,  0.3636,  0.3639],\n         [ 0.0107, -1.4033,  0.5778,  0.4441, -0.0658, -0.8724, -0.6508,\n          -1.0449,  0.0564,  0.2961]],\n\n        [[-1.8203, -0.0227,  0.6892,  0.0112,  0.6261,  0.0274, -0.3538,\n          -1.4725, -0.3154,  1.0137],\n         [ 0.4392, -0.4874,  0.1536,  0.5325,  1.7367, -0.5537,  1.2015,\n           0.4060, -0.2801,  0.3332],\n         [-1.7493, -0.4187, -0.5030,  0.6437,  0.3462, -0.7813, -0.8125,\n           0.8259,  0.0360,  0.0008]],\n\n        [[ 0.0995, -1.4944,  1.9550,  0.7636, -1.0913,  1.1989, -0.1107,\n           0.2074,  0.7975, -0.7645],\n         [-0.8593, -1.5094, -0.0455,  0.1928,  1.4133, -1.2078, -0.1066,\n           0.5333, -0.4069,  1.0011],\n         [-0.0667, -2.7385, -0.7673, -0.5962, -1.9605,  1.4199,  0.3532,\n           1.1660,  0.4277, -0.1369]],\n\n        [[-0.5397,  1.1403,  0.2739, -0.0221, -1.4910, -0.4309,  0.5053,\n          -0.3623, -0.3884,  0.3365],\n         [ 0.0967, -0.2206, -0.2204, -1.4200,  1.5322, -0.0264,  0.1607,\n           1.2313,  0.1810,  1.4376],\n         [ 0.0970,  1.1804,  0.1950, -0.3181, -1.3493, -0.3956, -0.4367,\n          -1.4018, -0.9502, -1.0948]]])"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 10); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始状态，共3个序列，2层，维度是20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.1467, -0.6165,  0.2578, -0.3161, -0.1763, -0.1667,  1.3993,\n          -0.0647,  0.2494, -0.2049,  0.1499,  1.3912, -0.2295, -0.2568,\n          -0.8759,  0.7645, -0.2468,  0.6890,  0.6483,  1.3278],\n         [-1.0142, -1.5736,  0.5107,  2.0680,  1.2704,  0.6716,  0.1626,\n          -0.8759, -0.6459, -0.9180, -0.2725, -0.7175,  0.5239, -0.2173,\n          -1.3671,  0.5428,  0.0473,  0.0568,  0.9350,  1.0221],\n         [ 1.2724,  0.9895,  1.1697,  1.1275,  0.3325, -0.3914, -0.0722,\n           0.4492, -1.0444, -1.1348,  1.1292, -1.3776,  0.3336, -0.5393,\n          -0.9897,  0.0647,  1.1800, -0.3506, -0.8704, -1.6343]],\n\n        [[-0.5512,  1.7977,  0.9653,  0.3691, -0.4722,  1.8211,  0.2733,\n           0.0794, -1.1279, -0.2973, -0.0541, -1.4686,  1.1699,  0.5713,\n          -0.2022, -1.0224, -0.1765,  1.0786, -0.6800,  0.2373],\n         [-0.5898, -1.2643,  0.2995,  0.5907,  0.9595, -0.8869, -0.6807,\n          -0.7182,  0.4525, -1.4235,  1.8818, -0.4123, -0.0281, -1.1159,\n           0.7143,  1.4111, -0.6679,  1.5994,  1.4619,  1.2365],\n         [-0.5238, -1.2531,  1.3022, -0.6528,  0.9190,  0.0468, -0.0773,\n           1.3024, -0.2849,  0.2624,  0.6278, -0.5448,  2.0409,  0.1247,\n           0.8893, -1.2669, -1.2305,  1.2996, -0.9589, -1.0706]]])"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "h0 = torch.randn(2, 3, 20); h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output是所有的RNN状态，大小为5*3*20；hn大小为2*3*20，为RNN最后一个状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hn = rnn(x, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.5584,  0.7450,  0.7515,  0.2207, -0.3389,  0.5826, -0.0202,\n          -0.0991, -0.3450,  0.1980,  0.1280, -0.5844,  0.7615,  0.2036,\n           0.2498, -0.5843,  0.0876,  0.6018, -0.1724,  0.3312],\n         [-0.4281, -1.1675,  0.0453,  0.2297,  0.6998, -0.4277, -0.5196,\n          -0.2244,  0.1568, -0.9400,  0.8813, -0.0868, -0.1761, -0.4002,\n           0.6112,  1.0781, -0.2811,  0.6485,  0.8611,  0.3393],\n         [-0.3270, -0.5938,  0.4756, -0.2269,  0.6389,  0.2558,  0.0614,\n           0.9865, -0.0110,  0.3368,  0.5804, -0.3555,  0.4066, -0.3482,\n           0.6340, -0.7100, -0.6215,  0.9279, -0.2670, -0.9842]],\n\n        [[-0.4218,  0.2798,  0.5486,  0.1766, -0.1472,  0.1510, -0.1905,\n          -0.2109,  0.0078,  0.2845,  0.1491, -0.1828,  0.5647,  0.0087,\n           0.3208, -0.3091,  0.1983,  0.4715,  0.2120,  0.3195],\n         [-0.3029, -0.8928,  0.0128,  0.0824,  0.4381, -0.2034, -0.3533,\n           0.0441,  0.1389, -0.5956,  0.4432,  0.0450, -0.2927, -0.2549,\n           0.4321,  0.7586, -0.0594,  0.4452,  0.5836, -0.0407],\n         [-0.2409, -0.3118,  0.1757, -0.0065,  0.2616,  0.3134,  0.1526,\n           0.6156,  0.1695,  0.3035,  0.5010, -0.2591, -0.0673, -0.3409,\n           0.4007, -0.4585, -0.4486,  0.7270, -0.0425, -0.7464]],\n\n        [[-0.3904,  0.1151,  0.4190,  0.1564,  0.0163,  0.0360, -0.2075,\n          -0.2648,  0.1203,  0.3218,  0.1613,  0.0548,  0.3808,  0.0826,\n           0.3185, -0.1280,  0.1769,  0.4485,  0.4015,  0.2234],\n         [-0.0636, -0.5219,  0.0747,  0.0338,  0.3262, -0.1245, -0.1678,\n           0.1722,  0.1764, -0.2939,  0.1949,  0.1278, -0.3512, -0.1550,\n           0.2939,  0.5097,  0.0595,  0.3644,  0.4356, -0.1724],\n         [-0.2458, -0.0769,  0.1706,  0.1009,  0.0976,  0.2859,  0.1732,\n           0.2954,  0.2168,  0.2787,  0.4451, -0.1203, -0.2311, -0.2600,\n           0.2270, -0.3863, -0.3910,  0.5862,  0.0846, -0.4717]],\n\n        [[-0.2413,  0.0335,  0.2874,  0.1846, -0.0020, -0.0008, -0.0994,\n          -0.2503,  0.1888,  0.3419,  0.1149,  0.0276,  0.0846,  0.0382,\n           0.1894, -0.0610,  0.0257,  0.4562,  0.3052,  0.1348],\n         [ 0.0151, -0.1842,  0.1747,  0.0947,  0.3192, -0.0422, -0.0755,\n           0.1536,  0.2056, -0.0504,  0.0899,  0.1837, -0.3707, -0.0543,\n           0.2464,  0.2861,  0.0227,  0.3365,  0.3626, -0.1639],\n         [-0.2682,  0.0845,  0.1939,  0.0825, -0.0050,  0.2058,  0.1595,\n           0.1534,  0.1807,  0.1702,  0.3648, -0.1597, -0.3194, -0.2717,\n           0.0916, -0.3569, -0.3923,  0.4871,  0.0466, -0.2790]],\n\n        [[-0.1765, -0.0334,  0.2632,  0.2015,  0.0668, -0.0052, -0.0894,\n          -0.2180,  0.1517,  0.2934,  0.1580,  0.0670, -0.0667,  0.0823,\n           0.1841,  0.0103,  0.0151,  0.4317,  0.2388,  0.0480],\n         [ 0.0221,  0.0946,  0.2801,  0.0931,  0.3072,  0.0321,  0.0091,\n           0.1027,  0.1712,  0.1340,  0.0630,  0.1989, -0.3528, -0.0484,\n           0.2209,  0.1015, -0.0174,  0.2646,  0.3666, -0.1498],\n         [-0.2292, -0.0043,  0.2320,  0.1041,  0.0377,  0.1468,  0.0719,\n          -0.0365,  0.1601,  0.0923,  0.2903, -0.1216, -0.2152, -0.1764,\n           0.1037, -0.1919, -0.2692,  0.4565,  0.0798, -0.1452]]],\n       grad_fn=<CatBackward>)"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[-0.0857,  0.1412, -0.2785,  0.1061,  0.1241, -0.1227, -0.0866,\n           0.0898,  0.0458,  0.0079, -0.0340,  0.1254, -0.1609, -0.2855,\n           0.1338, -0.2518,  0.0936, -0.2039,  0.1216, -0.1918],\n         [ 0.2979, -0.3138,  0.2738,  0.0807, -0.0743,  0.3784, -0.0882,\n           0.0910,  0.5830,  0.3327, -0.4801,  0.1771, -0.5598,  0.2996,\n           0.3533,  0.0559,  0.5862, -0.2444, -0.0503, -0.5810],\n         [ 0.1657,  0.2654, -0.1810,  0.0502,  0.0726, -0.0892,  0.1264,\n           0.0524, -0.0699,  0.0268, -0.0855, -0.2725,  0.0741, -0.0909,\n          -0.1576, -0.0796, -0.0048, -0.1826,  0.0671,  0.0258]],\n\n        [[-0.1765, -0.0334,  0.2632,  0.2015,  0.0668, -0.0052, -0.0894,\n          -0.2180,  0.1517,  0.2934,  0.1580,  0.0670, -0.0667,  0.0823,\n           0.1841,  0.0103,  0.0151,  0.4317,  0.2388,  0.0480],\n         [ 0.0221,  0.0946,  0.2801,  0.0931,  0.3072,  0.0321,  0.0091,\n           0.1027,  0.1712,  0.1340,  0.0630,  0.1989, -0.3528, -0.0484,\n           0.2209,  0.1015, -0.0174,  0.2646,  0.3666, -0.1498],\n         [-0.2292, -0.0043,  0.2320,  0.1041,  0.0377,  0.1468,  0.0719,\n          -0.0365,  0.1601,  0.0923,  0.2903, -0.1216, -0.2152, -0.1764,\n           0.1037, -0.1919, -0.2692,  0.4565,  0.0798, -0.1452]]],\n       grad_fn=<ViewBackward>)"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义网络模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在PyTorch开发中，我们经常需要实现一个新的网络结构，例如全连接层后接RNN层再接全连接层。我们希望所有的计算、求导都能在定制的网络上进行。为此PyTorch提供了一个简洁的工具————将定制的网络写成一个类，它需要继承基类nn.Module并实现其中的构造函数和前向计算函数forward。PyTorch将根据前向计算得到网络的结构，然后自动实现反向传播求导的过程backward。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在下面的代码中，我们实现了一个简单的网络FirstNet，它由一个全连接层、Dropout层和RNN层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定制网络为一个class，继承nn.Module基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNet(nn.Module):\n",
    "    # 构造函数，输入数据的维度input_dim，全连接后的维度rnn_dim，RNN状态的维度state_dim\n",
    "    def __init__(self, input_dim, rnn_dim, state_dim):\n",
    "        super(FirstNet, self).__init__()  # 调用父类nn.Module.__init__()方法\n",
    "        self.linear = nn.Linear(in_features=input_dim, out_features=rnn_dim)  # 全连接层\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Dropout层，置零概率为0.3\n",
    "        self.rnn = nn.GRU(input_size=rnn_dim, hidden_size=state_dim, batch_first=True)  # 单层单向GRU\n",
    "\n",
    "    # 前向计算函数，x大小为batch*seq_len*input_dim，为长度是seq_len的输入序列\n",
    "    def forward(self, x):\n",
    "        rnn_input = self.dropout(self.linear(x))  # 对全连接层的输出进行dropout，结果维度为batch*seq_len*rnn_dim\n",
    "        _, hn = self.rnn(rnn_input)  # GRU的最后一个状态，大小为1*batch*state_dim\n",
    "        # 交换第0、1维，输出维度为batch*1*state_dim\n",
    "        return hn.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0.5904,  0.3055,  0.4063,  0.0102, -0.3069, -0.0033,  0.2001,\n           0.0980,  0.0308,  0.2591, -0.3474, -0.0509,  0.4294, -0.3988,\n          -0.1944]],\n\n        [[ 0.2186,  0.5173,  0.4278,  0.2601,  0.1865,  0.2927, -0.1015,\n          -0.0396,  0.6598,  0.1779,  0.4690,  0.1746, -0.3549,  0.2339,\n           0.5734]],\n\n        [[-0.2472,  0.1883, -0.0074, -0.2397, -0.1368,  0.0363, -0.2424,\n          -0.1632, -0.0043,  0.1990, -0.1810,  0.0441, -0.5758,  0.2696,\n          -0.1659]]], grad_fn=<TransposeBackward0>)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 获取网络实例\n",
    "net = FirstNet(input_dim=10, rnn_dim=20, state_dim=15)\n",
    "x = torch.randn(3, 5, 10)  # batch是第0维，每个序列长度5，维度为10。具体为：共3个batch，每个batch中有5个长度为10的序列\n",
    "res = net(x); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 1, 15])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在给FirstNet加入训练任务和损失函数。这个机器学习任务是进行回归，即对每个序列输出一个实数预测它的某种性质。损失函数时均方差。训练数据有n个序列及对应的真值y。我们不需要在代码中实现backward函数，因为PyTorch会自动根据当前批次动态地得到计算图，并计算出每个参数的导数。每次计算导数前必须执行导数清零函数zero_grad()，因为PyTorch中张量的导数不会在一个批次计算后自动归零。此外，PyTorch的train()和eval()函数可以将网络设置成训练模式和测试模型，并根据设定的模式对Dropout进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FirstNet(\n  (linear): Linear(in_features=10, out_features=20, bias=True)\n  (dropout): Dropout(p=0.3)\n  (rnn): GRU(20, 15, batch_first=True)\n)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "net.train()  # 将FirstNet置为训练模式（启用Dropout）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()  # 如果有GPU，执行此语句将FirstNet的参数放入GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机定义训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(30, 5, 10)  # 共30个序列，每个序列长度为5，维度为10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(30, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度下降SGD优化器，学习率0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for batch_id in range(10):\n",
    "    # 获取当前批次的数据，batch_size=3\n",
    "    x_now = x[batch_id * 3: (batch_id + 1) * 3]\n",
    "    y_now = y[batch_id * 3: (batch_id + 1) * 3]\n",
    "    res = net(x_now)  # RNN结果res，维度为3*1*15\n",
    "    y_hat, _ = torch.max(res, dim=2)  # Max-pooling预测张量y_hat，维度3*1\n",
    "    # 均方差损失函数\n",
    "    loss = torch.sum(((y_now - y_hat) ** 2.0)) / 3\n",
    "    optimizer.zero_grad()  # 将net中的所有张量的导数清零\n",
    "    loss.backward()  # 自动实现反向传播\n",
    "    optimizer.step()  # 按优化器的规则沿导数反方向移动每个参数\n",
    "net.eval()  # 训练完成后，将FirstNet置为测试模式（即Dropout不置零，也不删除神经元）\n",
    "y_pred = net(x)  # 获得测试模式下的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitmrccondaf2fda53e5a1a4db7b763f220d2468ca5",
   "display_name": "Python 3.6.5 64-bit ('MRC': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}