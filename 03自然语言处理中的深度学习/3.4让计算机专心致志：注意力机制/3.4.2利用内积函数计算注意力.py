# -*- coding: utf-8 -*-
# -*- author: JeremySun -*-
# -*- dating: 20/5/22 -*-

"""
注意力机制最早是在机器视觉领域被提出的。它基于一个直观的想法，即人类在识别图像中物体时并不是观察每个像素，
而是集中注意力在图像中某些特定的部分，观察别的物体时，注意力有移动到另外的领域。

同样的道理也可以推广到文本理解中。因此，深度学习在处理文本信息时，可以根据当前的需要将权重集中到某些词语上，
而且随着时间推移，这一权重可以自动调整。这就是自然语言理解里的注意力机制。

注意力机制的输入包括两个部分：
	1、被注意的对象，为一组向量{a1, a2, ..., an}，如输入文本的词向量；
	2、一个进行注意的对象，为一个向量x。

向量x需要对{a1, a2, ..., an}进行总结，但是x对每个ai的注意力都不一样。注意力取决于从向量x的角度给被注意
对象的打分，更高的分值代表该对象更应该被关注。然后将打分用softmax归一化后并对{a1, a2, ..., an}计算加权和，
得到最终的注意力向量c。

注意力机制中的打分过程是通过注意力函数（attention function）实现的。注意力函数没有固定的形式，
只需要对两个输入向量得到一个相似度分数即可，例如使用内积函数。

向量c是{a1, a2, ..., an}的线性组合，但是权重来自x和每个ai的交互，即注意力的计算。

"""

import torch
import torch.nn.functional as F


# a：被注意的向量组，batch*m*dim
# x：需要进行注意力计算的向量组，batch*n*di
def attention(a, x):
    scores = x.bmm(a.transpose(1, 2))  # 内积计算注意力分数，结果维度为batch*n*m
    alpha = F.softmax(scores, dim=-1)  # 对最后一维进行了softmax
    attended = alpha.bmm(a)
    return attended


# 测试
batch = 10
m = 20
n = 30
dim = 15

a = torch.randn(batch, m, dim)
x = torch.randn(batch, n, dim)
res = attention(a, x)
print(res.shape)
# torch.Size([10, 30, 15])

"""
上述代码通过torch.bmm函数计算出内积值，然后用softmax将分数归一化为概率alpha，
最后与向量组a计算加权和。这样每一个x中的向量x[i,j]都得到了一个对应的dim维注意
力向量attended[i,j]。

alpha.bmm(a)：
tensor([[[-1.2777,  0.1308,  0.5422,  0.9575,  1.2350,  0.8538],
         [-0.1269,  1.3519, -0.1244,  0.3560,  0.6835,  0.7052],
         [ 1.5126,  0.8247,  0.7597,  0.6718,  1.8017,  0.7520],
         [-1.1750,  0.8077,  0.1167,  0.6678,  0.7483,  0.8485],
         [-1.6719,  0.0429,  0.4184,  0.9402,  1.1106,  0.8011]],

        [[ 0.5245, -0.7444,  0.0861, -0.5164, -0.3606,  0.4311],
         [-0.1237, -0.2702,  0.0902, -0.1064, -0.1369,  0.3675],
         [-0.9055,  1.2820,  1.1149,  0.3307, -0.8892, -1.3091],
         [-0.7839,  1.1383,  1.0726,  0.1922, -0.9103, -1.2446],
         [-1.4463,  0.1841, -0.1396, -0.4894,  0.1552,  0.3665]],

        [[ 0.1814, -0.4094, -0.1681,  0.0879, -1.4391,  0.5162],
         [-0.2104, -1.5577, -0.1017, -0.1119, -0.4862,  0.0392],
         [ 0.4337, -2.1029,  0.9022,  0.0635,  0.2701, -0.4824],
         [ 0.8129, -0.7692,  0.7594,  0.2887, -0.7658,  0.0731],
         [ 0.1085, -0.6910, -0.1297,  0.0426, -1.2185,  0.3965]]])

x：
tensor([[[-0.0428, -1.4757,  0.1118, -0.0216, -0.2263,  2.0199],
         [ 0.4707, -0.3995,  0.2690, -0.1331, -1.1342,  0.9969],
         [ 1.1943, -1.3877,  0.6420, -0.2625,  1.1440, -0.3311],
         [-0.8439, -0.0825,  0.4312,  0.3952, -1.3376,  1.6369],
         [-1.8798, -1.2812, -0.9738,  0.1481,  0.5884, -0.6801]],

        [[ 0.3433, -1.3652, -0.6973, -0.2658, -0.3187,  0.3185],
         [ 0.4436, -0.3864, -0.0599, -0.0949,  1.7543, -1.1337],
         [-1.2018,  1.6452,  0.2310,  0.5143, -0.7404, -1.0318],
         [ 0.2353,  0.9189, -1.2240,  0.6124, -2.3176, -1.2422],
         [-0.4351,  1.2597, -0.2091, -0.6408,  0.2874,  0.7594]],

        [[-0.2734,  0.8880, -0.6958,  0.0516, -0.7924,  1.4932],
         [-1.0932, -0.4591, -0.2335, -0.6173, -0.8711,  0.8345],
         [-0.7876, -1.6546,  1.3631,  1.0302, -1.1578,  1.0829],
         [ 1.4335,  0.5851, -0.2613, -1.3421, -0.4214,  0.4111],
         [ 1.2593, -0.4362, -2.0440, -0.9495, -1.4677, -0.9775]]])

"""
