# -*- coding: utf-8 -*-
# -*- author: JeremySun -*-
# -*- dating: 20/5/19 -*-

# 含参加权和也是 一种将多个词向量变成一个文本向量的常用方法。如果输入为n个向量，使用简单平均，即为平均池化。
# 但在有些情况下，我们希望权重能根据向量之间的关系确定，并且可以优化，这就是含参加权和。

import torch
import torch.nn as nn
import torch.nn.functional as F


class WeightedSum(nn.Module):
    def __init__(self, word_dim):
        """
        :param word_dim: 输入的词向量维度word_dim
        """
        super(WeightedSum, self).__init__()
        self.b = nn.Linear(in_features=word_dim, out_features=1)  # 参数张量


    def forward(self, x):
        """
        :param x: 输入tensor，维度为batch*seq_len*word_dim
        :return: res，维度是batch*word_dim
        """
        scores = self.b(x)  # 内积得分，维度是batch*seq_len*1
        weights = F.softmax(input=scores, dim=1)  # softmax运算，结果维度是batch*seq_len*1
        res = torch.bmm(x.transpose(1, 2), weights)  # 用矩阵乘法实现加权和，结果维度是batch*word_dim*1
        res = res.squeeze(2)  # 删除最后一维，结果维度是batch*word_dim
        return res


# 测试
batch = 10
seq_len = 20
word_dim = 50

x = torch.randn(batch, seq_len, word_dim)
weighted_sum = WeightedSum(word_dim=word_dim)
res = weighted_sum(x)
print(res)
print(res.shape)

"""
tensor([[ 0.1423,  0.0007, -0.0361,  0.1111, -0.2376,  0.2470,  0.0812,  0.1867,
         -0.1733,  0.6023,  0.3441, -0.0684,  0.1557, -0.0204, -0.4937,  0.4434,
         -0.3778, -0.0203, -0.0465, -0.1293,  0.0057, -0.6222,  0.4629,  0.0606,
         -0.6089,  0.3916,  0.3247, -0.1998,  0.1170,  0.0786,  0.0025,  0.0477,
         -0.4197,  0.1883,  0.1225, -0.0785,  0.0359, -0.5547,  0.3711,  0.3769,
         -0.3452,  0.3829, -0.5463, -0.1906, -0.2991,  0.0584,  0.6244,  0.0684,
          0.0000, -0.1022],
        [ 0.2135, -0.4949,  0.0860,  0.1571,  0.0395,  0.4169, -0.5832, -0.1134,
         -0.3849,  0.2643, -0.4066,  0.0514, -0.0078,  0.2307, -0.3282, -0.3674,
          0.1304, -0.6461,  0.1296,  0.1765,  0.3265, -0.0069,  0.0204, -0.3459,
         -0.3340,  0.3353,  0.0420, -0.5767, -0.4201, -0.0922, -0.0181,  0.4935,
         -0.5569,  0.2191, -0.0787, -0.0534,  0.0592, -0.6598, -0.2935,  0.3573,
          0.2787,  0.1600,  0.0998, -0.0505, -0.0213,  0.4108, -0.1987,  0.2610,
          0.3957,  0.2642],
        [-0.4009, -0.1866, -0.1587, -0.0673, -0.3173,  0.1743, -0.0674, -0.3588,
         -0.1125, -0.3099, -0.1543, -0.0965,  0.1457, -0.2944, -0.2951,  0.3178,
         -0.2058, -0.1039,  0.7134,  0.0819, -0.1673,  0.1170, -0.4067, -0.1156,
         -0.2557, -0.0753,  0.0585,  0.2050,  0.2611, -0.3654,  0.1217,  0.3052,
         -0.1629, -0.1019, -0.4904, -0.4000,  0.1188,  0.2460,  0.0402,  0.1816,
         -0.1695,  0.3433, -0.1108, -0.2456, -0.1965,  0.1651,  0.0794, -0.2004,
          0.1167,  0.1501],
        [-0.6636, -0.5875, -0.2894, -0.1690,  0.4892,  0.2702, -0.0080, -0.2558,
          0.1631,  0.1859,  0.2440, -0.0540, -0.0233, -0.0707, -0.5591,  0.3810,
          0.3975,  0.1452,  0.2203, -0.3432,  0.2072, -0.2621,  0.0307, -0.0311,
         -0.2845, -0.1839, -0.2050, -0.2237, -0.1852,  0.0743, -0.2531, -0.0646,
         -0.3564,  0.0570,  0.1969, -0.1030, -0.3376, -0.1432, -0.1641,  0.4601,
          0.1073,  0.3221, -0.4619, -0.2452, -0.5884, -0.0335,  0.2328,  0.5085,
          0.7098,  0.2574],
        [-0.1691, -0.0250, -0.0325,  0.0300, -0.1509, -0.0053,  0.3476,  0.1724,
          0.3733,  0.3022,  0.0861,  0.0866,  0.6056,  0.1575,  0.0067, -0.2088,
         -0.2353, -0.1090,  0.0281, -0.0141, -0.4565, -0.0209,  0.1941, -0.3597,
          0.2822,  0.2276, -0.3150, -0.5552, -0.1639,  0.2362,  0.3872,  0.1619,
          0.0544,  0.3073,  0.0942,  0.5342,  0.1617,  0.2849, -0.0310, -0.1591,
         -0.1578,  0.0779, -0.0276, -0.0555,  0.0641, -0.0102, -0.0285, -0.6064,
          0.2271,  0.1056],
        [-0.4868,  0.2732,  0.1025, -0.2634, -0.0922,  0.0626, -0.2390, -0.2100,
          0.3190, -0.1225,  0.1539,  0.1990,  0.1764, -0.1973, -0.2540,  0.2492,
         -0.1484, -0.1176, -0.1586, -0.0141,  0.0599, -0.4870, -0.3414,  0.0172,
         -0.4152,  0.4260, -0.2586, -0.1621, -0.0878, -0.3603, -0.0748,  0.0488,
          0.1023, -0.1873,  0.8279,  0.4283,  0.0446, -0.1581,  0.5860,  0.0729,
          0.2343, -0.2419, -0.0244,  0.2599, -0.4936,  0.0225,  0.7058,  0.0444,
         -0.0008, -0.0442],
        [-0.6127,  0.1058, -0.1555,  0.0210, -0.0820,  0.1587,  0.1527, -0.2010,
          0.1019,  0.2170, -0.1549,  0.1482,  0.2409, -0.2997,  0.1790,  0.0013,
         -0.4389,  0.1669,  0.2870,  0.1995, -0.4086,  0.1437, -0.4546, -0.0648,
         -0.1971,  0.0704, -0.2876,  0.2614, -0.3780, -0.3356,  0.1437,  0.2778,
          0.2591, -0.2086,  0.0772, -0.3924, -0.2510,  0.1867, -0.0106,  0.0453,
          0.3761,  0.0521,  0.0283,  0.0338, -0.1640,  0.2187,  0.1886, -0.3005,
          0.2490,  0.0798],
        [-0.3395,  0.1882, -0.2285,  0.0441,  0.0118,  0.1082,  0.2909, -0.2391,
          0.0384,  0.4046,  0.0515, -0.2743,  0.0930,  0.0331,  0.3925, -0.0653,
         -0.2794, -0.0836,  0.1598, -0.1293,  0.0960, -0.2023, -0.2591, -0.2576,
         -0.3804,  0.0470,  0.0928, -0.3669, -0.0503, -0.0964,  0.3830,  0.3825,
         -0.1811, -0.1639,  0.4139, -0.2757,  0.2639, -0.0540, -0.2649,  0.2602,
         -0.4673,  0.2361, -0.0902, -0.5070, -0.1445,  0.0362,  0.1005, -0.0093,
          0.2738,  0.1454],
        [ 0.0389, -0.3250, -0.1183, -0.0535, -0.0982,  0.0472,  0.2488,  0.2318,
         -0.4138, -0.3575,  0.2513,  0.4945,  0.0629, -0.5331,  0.1125,  0.6311,
         -0.1561, -0.5876,  0.3608, -0.2167,  0.3923,  0.0352, -0.6076, -0.3116,
         -0.2385,  0.3409, -0.0810,  0.1626, -0.0253,  0.0855,  0.1316, -0.0408,
          0.4068,  0.6240, -0.0023, -0.1713,  0.2497, -0.0658, -0.0071,  0.3074,
          0.0302,  0.0984, -0.0202, -0.2316,  0.1292,  0.2593, -0.1090, -0.0218,
         -0.3013,  0.3470],
        [-0.3816,  0.1204, -0.3497, -0.3570, -0.1396,  0.0229, -0.4273,  0.0726,
         -0.2523, -0.0511, -0.1723,  0.0135,  0.2257, -0.0658,  0.2283,  0.1256,
         -0.4102, -0.2702,  0.0068,  0.0402,  0.1473, -0.2624, -0.2490,  0.1054,
          0.2263, -0.0877, -0.1621, -0.4568, -0.2776,  0.2875,  0.3308,  0.2128,
          0.3328,  0.2541,  0.0569,  0.6873,  0.3243,  0.0628,  0.1845,  0.0032,
          0.0870,  0.0154, -0.0616,  0.1973,  0.1463, -0.2062,  0.1348, -0.3089,
         -0.0944,  0.0311]], grad_fn=<SqueezeBackward1>)
torch.Size([10, 50])
"""