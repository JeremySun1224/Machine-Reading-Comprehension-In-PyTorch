自然语言处理的一个重要任务（语言模型），就是给那些合法语句赋予较大的概率，
而给罕见或不可能出现的句子赋予很小的概率。如果这个概率分配得当，计算机就可
以像人类一样识别与生成合乎语法、符合正常思维的句子。

所以，对于任何需要生成答案语句的MRC算法都必须建立语言模型，给语句赋予合适
的概率。

2.4.1 N元模型
	
	N元模型的出发点是，下一个词的概率只和之前n-1个单词有关。一般而言，训练
	一个领域的语言模型需要使用该领域内的文本资料。

	但是，因为句子的长度不是固定的，所以N元模型在计算出概率后所有句子的概
	率和并不为1。因此，在实际计算中，N元模型需要在每个句子前后加上开始和结
	束标识符。如将“我们今天来吃饭”变成“<s>我们|今天|来|吃饭</s>。<s>和</s>
	表示句子的开始和结束，也作为词汇表的一部分。可以证明，在加入句子结束的
	标识符</s>后，所有长度的句子的语言模型概率总和等于1。

	有的时候，合法句子的部分单词如果未在文中出现，就会造成概率没有定义。为了
	解决这个问题，研究者提出来拉普拉斯平滑（laplace smoothing）的概念。其
	原理是在计数中加上一个平滑项1（也可以是一个给定的值K，称为add-k smoothing）。
	这样，所有词对应的概率都大于0就可以解决这个问题。