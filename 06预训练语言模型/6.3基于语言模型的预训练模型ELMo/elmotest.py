# -*- coding: utf-8 -*-
"""ELMoTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dyhggtgth77MwaOTqjEFQ5YCt2sUXtC5
"""

!nvidia-smi

# ELMo预训练模型的使用

# Commented out IPython magic to ensure Python compatibility.
# %cd ./drive/My\ Drive/PyTorch

!pip install allennlp

!pwd

import torch
from torch import nn
import torch.nn.functional as F
from allennlp.modules.elmo import Elmo, batch_to_ids
from allennlp.commands.elmo import ElmoEmbedder
from allennlp.nn.util import remove_sentence_boundaries

# 预训练模型下载

options_file = "https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json"
weight_file = "https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5"

# 获得ELMo编码器的类

elmo_bilm = ElmoEmbedder(options_file=options_file, weight_file=weight_file).elmo_bilm
elmo_bilm.cuda()
sentences = [['Taday', 'is', 'sunny', '.'], ['Hello', '!']]

elmo_bilm.cuda()

# 获得所有单词的字符ID，维度为batch_size(2)*max_sentence_len(4)*word_len(50)

character_ids = batch_to_ids(sentences).cuda();character_ids

character_ids.shape

# 获得ELMo输出

bilm_output = elmo_bilm(character_ids);bilm_output

# ELMo编码

layer_activations = bilm_output['activations'];layer_activations

# 每个位置是否有单词

mask_with_bos_eos = bilm_output['mask'];mask_with_bos_eos

# 去掉ELMo加上的句子开始和结束符

without_bos_eos = [remove_sentence_boundaries(layer, mask_with_bos_eos) for layer in layer_activations];without_bos_eos

# 获得三层ELMo编码，每层1024维，维度为3*batch_size(2)*max_sentence_len(4)*1024

all_layers = torch.cat([ele[0].unsqueeze(0) for ele in without_bos_eos], dim=0);all_layers

# 求加权和时每层的权重参数

s = nn.Parameter(torch.Tensor([1., 1., 1.]), requires_grad=True).cuda();s

# 权重和为1

s = F.softmax(s, dim=0);s

# 求加权和时的相乘因子gamma

gamma = nn.Parameter(torch.Tensor(1, 1), requires_grad=True).cuda();gamma

# 获得ELMo编码，维度为batch_size(2)*max_sentence_len(4)*1024

res = (all_layers[0]*s[0] + all_layers[1]*s[1] + all_layers[2]*s[2]) * gamma;res

res.shape

"""##### 经过大规模语言模型预训练，ELMo获得了有效的上下文表示。与基于机器翻译的CoVe相比，ELMo能够充分利用预训练模型的参数。"""