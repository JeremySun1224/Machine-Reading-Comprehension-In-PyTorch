{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;CoVe基于在机器翻译领域中广泛应用的编码器-解码器结构：编码器求出源语言文本的向量表示，然后由解码器生成目标语言的语句。由于机器翻译需要对目标语言准确表达源语言文本的语义，所以编码器所生成的向量表示包含了对源文本的语义的理解。因此，CoVe提出，经过大规模翻译数据训练的编码器可以用在与源语言文本理解相关的任务中，即可以将预训练的机器翻译编码器作为其他NLP任务的编码层的一部分。\n",
    "\n",
    "<br>&emsp;&emsp;CoVe采用的机器翻译模型经典的序列到序列架构模型。CoVe选取的任务是将英文翻译成德文。它首先使用GloVe得到每个词的向量表示，之后将GloVe编码输入双层双向循环神经网络LSTM中，得到编码器的输出——600维上下文编码。解码层采用了双层单向循环神经网络LSTM，用于对每一步生成的代词进行解码。在机器翻译模型经过大规模数据预训练后，其编码器部分可以产生任何文本的上下文编码CoVe(w)，然后将这个上下文编码和GloVe编码拼接，输入到目标任务的下游网络中。实验表明，在使用机器翻译预训练模型CoVe后，在情感分析、问题分类、语句关系和机器阅读理解任务中，相关表现均有所提升。这得益于机器翻译的预训练使得编码器可以对源语言进行有效的语义分析。\n",
    "\n",
    "<br>&emsp;&emsp;下面的代码展示了如何使用CoVe预训练模型。首先从GitHub上下载CoVe的源代码库并安装所需的软件包。在使用时，给定GloVe词表以及每个句子中每个单词的编号。最后，得到相应的CoVe编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载GitHub上CoVe的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitmrccondaf2fda53e5a1a4db7b763f220d2468ca5",
   "display_name": "Python 3.6.5 64-bit ('MRC': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}